{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "#_PARENT_DIR = dirname(dirname(dirname(abspath(__file__))))\n",
    "#sys.path.append(_PARENT_DIR)\n",
    "#from utils._utils import parse_celonis_url, get_user_data, get_logger              ### <--\n",
    "from pycelonis import get_celonis\n",
    "import argparse\n",
    "#logger = get_logger()\n",
    "def unify_tables(url, api_token, datapool, tables, tableSchema, function = 'null'):\n",
    "    \n",
    "    #loop through defined tables\n",
    "    #for table_name in tables:\n",
    "       \n",
    "    c = get_celonis(url, api_token)\n",
    "    #get_user_data(c, 'Table Unifier')\n",
    "    #url_options= parse_celonis_url(url)                                            ### <--\n",
    "    #pool = c.pools.find(url_options['id'])                                         ### <--\n",
    "    \n",
    "    pool = c.pools.find(datapool)                                                   ### <--\n",
    "\n",
    "    minReq = []\n",
    "    maxReq = []\n",
    "    sameTables = []\n",
    "    schemalist = tableSchema                                                        # added to extract required table schemas (in python list format)\n",
    "    columns = ''\n",
    "    select = ''\n",
    "    \n",
    "    # create or find global data job \n",
    "    try:\n",
    "        global_dj = pool.data_jobs.find('Unify Tables')\n",
    "    except:\n",
    "        global_dj = pool.create_data_job(name = 'Unify Tables')\n",
    "    \n",
    "    #loop through defined tables\n",
    "    for table_name in tables:\n",
    "    \n",
    "        # find tables to unify\n",
    "        x = 0\n",
    "        for schema in global_dj.tables:\n",
    "            for table in schema:\n",
    "                x = x + 1\n",
    "                if table['schemaName'] not in schemalist:\n",
    "                    continue\n",
    "                if table['name'] == table_name:\n",
    "                    sameTables.append(table)\n",
    "        \n",
    "        if len(sameTables) == 0:\n",
    "            print('TABLE', table_name, 'CAN NOT BE UNIFIED! Not existing in source systems!')\n",
    "            global_dj.create_transformation(name = \"Unify \" + table_name, statement = '-- no existing tables to unify in defined source systems')\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # create minimal column requirement\n",
    "            for col in sameTables[0]['columns']:\n",
    "                try:\n",
    "                    if col in sameTables[1]['columns']:                                              # ERROR CHECKING !\n",
    "                        minReq.append(col)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            \"\"\"\n",
    "            #only required if function = 'min'\n",
    "\n",
    "            if len(sameTables) > 2:\n",
    "                for table in sameTables[2:]:\n",
    "                    for col in minReq:\n",
    "                        if col not in table['columns']:\n",
    "                            minReq.pop(col)\n",
    "                # create transformation statement for minimal table requirement\n",
    "\n",
    "            if function == 'min':\n",
    "                for col in minReq:\n",
    "                    if(minReq.index(col) == 0):\n",
    "                        columns += '{}\\n'.format(col['name'])\n",
    "                    else:\n",
    "                        columns += ',{}\\n'.format(col['name'])\n",
    "                for table in sameTables:\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            \"\"\"            \n",
    "\n",
    "            # create transformation for statement for extension with null\n",
    "            if function == 'null':\n",
    "                #columns = ''\n",
    "                #select = ''\n",
    "                \n",
    "                for table in sameTables:\n",
    "                    for col in table['columns']:\n",
    "                        if col not in maxReq:\n",
    "                            maxReq.append(col)\n",
    "                for table in sameTables:\n",
    "                    for col in maxReq:\n",
    "                        if(maxReq.index(col) == 0):\n",
    "                            columns += \"'\" + table['schemaName'] + \"' \" + 'AS \"SCHEMA\"\\n' + ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                        else:\n",
    "                            columns += ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    columns = ''\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            # create transformation\n",
    "            #statement = 'DROP VIEW IF EXISTS \"{}_UNIFIED\";\\n\\nCreate VIEW \"{}_UNIFIED\" AS(\\n{}\\n);'.format(table_name, table_name, select)\n",
    "            statement = 'DROP VIEW IF EXISTS \"{}\";\\n\\nCreate VIEW \"{}\" AS(\\n{}\\n);'.format(table_name, table_name, select)                      ### <-- Unified\n",
    "            global_dj.create_transformation(name = \"Unify \" + table_name, statement = statement)\n",
    "            print('The union statement has been saved in the global data job \"Unify Tables\" for Table', table_name)\n",
    "            select = ''\n",
    "            sameTables = []\n",
    "            minReq = []\n",
    "            maxReq = []\n",
    "            \n",
    "            # reminder: datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://endresshauser.eu-3.celonis.cloud/'\n",
    "\n",
    "# Specify a valid API token for your Cloud Team.\n",
    "#Click on Team Settings -> Edit -> create or copy already inserted API key\n",
    "\n",
    "api_token = 'ZmE2MjY2Y2MtZGYxOC00ZGU4LTgzYmYtMjY2ZDUzZmY3ZmJiOklEdkZlRDZmNVNPd3FMRitLVmtxbVIrYko3MHBYMkNBdlZ0c1Y3K3M0bGYy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-02 10:11:23 - pycelonis: Login successful! The Application Key currently has access to 0 Analyses and to 4 Data Pools.\n"
     ]
    }
   ],
   "source": [
    "celonis = get_celonis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-02 10:11:25 - pycelonis: Login successful! The Application Key currently has access to 0 Analyses and to 4 Data Pools.\n"
     ]
    }
   ],
   "source": [
    "c = get_celonis(url, api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Pool, id e1bd9ca9-9d3b-44b4-9c17-ae345f663e27, name SAP ECC - Purchase to Pay>,\n",
       "<Pool, id 590652c7-87a4-46e6-a9e9-60c90cb7c4e3, name SAP ECC - Production Planning>,\n",
       "<Pool, id 98d9fc62-ccbf-4158-b89f-cb8a7a976a2d, name SAP ECC - Warehouse Management>,\n",
       "<Pool, id 60442433-3dbb-4ce6-8ed3-e9bb7238c614, name SAP ECC - Order to Cash>,]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celonis.pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapool = '98d9fc62-ccbf-4158-b89f-cb8a7a976a2d'\n",
    "\n",
    "pool = celonis.pools.find(datapool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DataConnection, id 52041618-a613-46bb-ad5f-435b9b7dc8c0, name SAP ECC - PSG-049>,\n",
       "<DataConnection, id 31149693-442a-4ab4-aa60-25dba4a1ccdf, name SAP ECC - PS1-033>,\n",
       "<DataConnection, id ac6cf012-2fdf-422b-8c15-5df275c548c2, name SAP ECC - PCA-042>,\n",
       "<DataConnection, id 130521d8-53e7-49d0-abf1-ef3e8300cce4, name SAP ECC - PIN-020>,\n",
       "<DataConnection, id e453d486-da03-497c-8c1d-1f21f4ce6ee2, name SAP ECC - PIN-024>,\n",
       "<DataConnection, id bec2b69a-269f-4aad-b9d8-312bd4e8e9d3, name SAP ECC - PP1-011>,\n",
       "<DataConnection, id e54616c8-7ec1-43d1-8c35-d7fe7367b019, name SAP ECC - PP2-004>,\n",
       "<DataConnection, id ba105640-52a9-4d05-bbbb-cb5fb3543b01, name SAP ECC - PP2-005>,\n",
       "<DataConnection, id 65e2be49-9011-46ab-ac51-b3d64c219255, name SAP ECC - PS1-007>,\n",
       "<DataConnection, id 79475b2f-4a61-4068-ac43-a950306c6adb, name SAP ECC - PS1-010>,\n",
       "<DataConnection, id 227a9d49-1773-4f80-9584-6e18ddf831d8, name SAP ECC - PS1-030>,\n",
       "<DataConnection, id 8c90f23b-1bad-4eaf-bd92-4889b10a25ab, name SAP ECC - PS1-032>,\n",
       "<DataConnection, id 0b9a4247-3de7-4885-90a7-5c2d2345dda8, name SAP ECC - PS1-035>,\n",
       "<DataConnection, id f2511b5d-acf5-45ef-a044-c1bef71d32ae, name SAP ECC - PS1-036>,\n",
       "<DataConnection, id 9047eff0-78bb-4816-aafc-eb5cdaeb640c, name SAP ECC - PS1-038>,\n",
       "<DataConnection, id bf43d527-b640-448f-8203-347cfe7ae47b, name SAP ECC - PS1-039>,\n",
       "<DataConnection, id 706a7a4a-8ce4-4c47-bac4-f1de73c9c43f, name SAP ECC - PS1-040>,\n",
       "<DataConnection, id ab4ec8a5-c774-4c51-a7fb-7fe154503eb4, name SAP ECC - PS1-044>,\n",
       "<DataConnection, id 899242bb-4a0c-438e-87c2-5a7ca5a739e4, name SAP ECC - PS1-090>,\n",
       "<DataConnection, id 0241cf52-4163-4dad-8efe-6e590151622b, name SAP ECC - PS1-900>,\n",
       "<DataConnection, id b5232af3-203e-4771-806b-0df39a494992, name SAP ECC - PSG-045>,\n",
       "<DataConnection, id 937520f5-1c42-4965-93a8-419d2a75a2f2, name SAP ECC - PSG-048>,]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.data_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-28 08:01:48 - pycelonis: Login successful! Hello josef.rieger@cbs-consulting.de\n",
      "2020-10-28 08:01:48 - pycelonis: Best matches: [(0.48, 'PSG 048 - SAP WM Tasks - Full Load'), (0.48, 'PSG 045 - SAP WM Tasks - Full Load'), (0.48, 'PS1 090 - SAP WM Tasks - Full Load ')]\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table DD02T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table DD03M\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table LTAK\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table LTAP\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table LTBK\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table LTBP\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table MKPF\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table MSEG\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table QALS\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table QAVE\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T163K\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T300T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T301T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T302T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T308\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T308T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T336T\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table USR02\n"
     ]
    }
   ],
   "source": [
    "# URL to the Cloud Data Pool where the tables to unify are hosted.\n",
    "url = 'https://endresshauser.eu-3.celonis.cloud/'\n",
    "\n",
    "# Specify a valid API token for your Cloud Team.\n",
    "#Click on Team Settings -> Edit -> create or copy already inserted API key\n",
    "\n",
    "api_token = 'MDkyZjRiOWMtYzQxNC00ZGJkLWJlNTctODUxYTUxMzE3OWFiOldYNkE4TUM4QTZndzc0Q0hpL3F6Ym9ESHhuejRvb0hUR3dkam5HRW1mdXhS'\n",
    "\n",
    "\n",
    "#To get the data pool id, you have to run the following code. To get the data\n",
    "#the ML User here called Global needs permission to request these informations\n",
    "#celonis = get_celonis()\n",
    "#celonis.pools by that you get the ID for the data pool \n",
    "\n",
    "# Specify a datapool -- WM Data Pool ID\n",
    "datapool = '98d9fc62-ccbf-4158-b89f-cb8a7a976a2d'\n",
    "\n",
    "# Specify the table names for which the tables should be merged. \n",
    "\n",
    "tables =    ['DD02T', 'DD03M', 'LTAK', 'LTAP', 'LTBK','LTBP','MKPF','MSEG',\n",
    "             'QALS', 'QAVE', 'T163K', 'T300T', 'T301T', 'T302T', 'T308', 'T308T', 'T336T', 'USR02', 'MARA', 'MARC'\n",
    "         ]\n",
    "\n",
    "\n",
    "# Specify 'min' if the unified table should be the minimal column requirement and 'null' if the not existing columns should be filled with null values.\n",
    "function = 'null'\n",
    "\n",
    "#Schema Information\n",
    "\n",
    "#'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_bec2b69a-269f-4aad-b9d8-312bd4e8e9d3',\n",
    "#'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_899242bb-4a0c-438e-87c2-5a7ca5a739e4',\n",
    "#'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_b5232af3-203e-4771-806b-0df39a494992',\n",
    "#'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_937520f5-1c42-4965-93a8-419d2a75a2f2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Specify the required table schemas in a python list\n",
    "#To acess the Schema Name you have to get the connection informations\n",
    "#pool.data_connections\n",
    "tableSchema = [\n",
    "'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_bec2b69a-269f-4aad-b9d8-312bd4e8e9d3',\n",
    "'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_899242bb-4a0c-438e-87c2-5a7ca5a739e4',\n",
    "'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_b5232af3-203e-4771-806b-0df39a494992',\n",
    "'98d9fc62-ccbf-4158-b89f-cb8a7a976a2d_937520f5-1c42-4965-93a8-419d2a75a2f2'\n",
    "]\n",
    "\n",
    "\n",
    "unify_tables(url = url, api_token = api_token, datapool = datapool, tables = tables, tableSchema = tableSchema, function = function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
