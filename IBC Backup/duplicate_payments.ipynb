{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superfluid Duplicate Checker Script (Internal Usage Only)\n",
    "## Script that can be used if streamlit APP fails or customization not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT PACKAGES #######\n",
    "\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycelonis import pql\n",
    "from pycelonis import get_celonis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:13:32 - pycelonis: Login successful! Hello Application Key, this key currently has access to 28 analyses.\n",
      "2020-05-18 10:13:33 - pycelonis: Best matches: [(0.97, 'DLH - Duplicate Payments 3.2M'), (0.97, 'DLH - Duplicate Payments 1.2M'), (0.83, 'DLH - Accounts Payable Data Model')]\n"
     ]
    }
   ],
   "source": [
    "#### CONNECT TO CELONIS AND DATAMODEL #######\n",
    "\n",
    "c = get_celonis(\n",
    ")\n",
    "# insert name of id of datamodel\n",
    "dm = c.datamodels.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define the Case Key of the Invoice #################\n",
    "\n",
    "case_key = ['\"BSEG\".\"MANDT\"', '\"BSEG\".\"BUKRS\"', '\"BSEG\".\"BELNR\"', '\"BSEG\".\"GJAHR\"', '\"BSEG\".\"BUZEI\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define the Data Columns that are supposed to be checked for duplicate entries #################\n",
    "\n",
    "data_columns = ['\"LFA1\".\"NAME1\"', '\"BKPF\".\"TS_BLDAT\"', '\"BKPF\".\"XBLNR\"', '\"BSEG\".\"WRBTR\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define the Filters to be appied on the DM  #################\n",
    "\n",
    "pql_filters = [\n",
    "    \"\"\"FILTER MATCH_ACTIVITIES(NODE_ANY['Clear Invoice'] ) = 1; \"\"\",\n",
    "    \"\"\"FILTER MATCH_ACTIVITIES(EXCLUDING['Set Payment Block','Create Credit Memo', 'Reverse Invoice'] ) = 1;\"\"\",\n",
    "    \"\"\"FILTER \"BSEG\".\"WRBTR_CONVERTED\" > 500;\"\"\",\n",
    "    \"\"\"FILTER PU_COUNT(DOMAIN_TABLE(\"BKPF\".\"XBLNR\"),\"BSEG\".\"MANDT\"||\"BSEG\".\"BUKRS\"||\"BSEG\".\"BELNR\"||\"BSEG\".\"GJAHR\"||\"BSEG\".\"BUZEI\") < 10; \"\"\",\n",
    "    \"\"\"FILTER LEN(\"BKPF\".\"XBLNR\") > 4; \"\"\",\n",
    "    \"\"\"FILTER ISNULL(\"LFA1\".\"VBUND\") = 1;\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424958, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_BSEG_MANDT</th>\n",
       "      <th>_BSEG_BUKRS</th>\n",
       "      <th>_BSEG_BELNR</th>\n",
       "      <th>_BSEG_GJAHR</th>\n",
       "      <th>_BSEG_BUZEI</th>\n",
       "      <th>LFA1_NAME1</th>\n",
       "      <th>BKPF_TS_BLDAT</th>\n",
       "      <th>BKPF_XBLNR</th>\n",
       "      <th>BSEG_WRBTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010</td>\n",
       "      <td>1TIA</td>\n",
       "      <td>1800000002</td>\n",
       "      <td>2019</td>\n",
       "      <td>008</td>\n",
       "      <td>Nettogehaelter</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>SALARY 02/19</td>\n",
       "      <td>323349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010</td>\n",
       "      <td>1TIA</td>\n",
       "      <td>1800000002</td>\n",
       "      <td>2019</td>\n",
       "      <td>010</td>\n",
       "      <td>Drejtoria Rajonale Tatimore Durres</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>SALARY 02/19</td>\n",
       "      <td>115148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010</td>\n",
       "      <td>1TIA</td>\n",
       "      <td>1800000003</td>\n",
       "      <td>2019</td>\n",
       "      <td>010</td>\n",
       "      <td>Drejtoria Rajonale Tatimore Durres</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>SALARY 03/19</td>\n",
       "      <td>114936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010</td>\n",
       "      <td>1TIA</td>\n",
       "      <td>1800000003</td>\n",
       "      <td>2019</td>\n",
       "      <td>008</td>\n",
       "      <td>Nettogehaelter</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>SALARY 03/19</td>\n",
       "      <td>322773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010</td>\n",
       "      <td>1TIA</td>\n",
       "      <td>1800000004</td>\n",
       "      <td>2019</td>\n",
       "      <td>009</td>\n",
       "      <td>Drejtoria Rajonale Tatimore Durres</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>SALARY 04/19</td>\n",
       "      <td>132482.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _BSEG_MANDT _BSEG_BUKRS _BSEG_BELNR _BSEG_GJAHR _BSEG_BUZEI  \\\n",
       "0         010        1TIA  1800000002        2019         008   \n",
       "1         010        1TIA  1800000002        2019         010   \n",
       "2         010        1TIA  1800000003        2019         010   \n",
       "3         010        1TIA  1800000003        2019         008   \n",
       "4         010        1TIA  1800000004        2019         009   \n",
       "\n",
       "                           LFA1_NAME1 BKPF_TS_BLDAT    BKPF_XBLNR  BSEG_WRBTR  \n",
       "0                      Nettogehaelter    2019-02-01  SALARY 02/19    323349.0  \n",
       "1  Drejtoria Rajonale Tatimore Durres    2019-02-01  SALARY 02/19    115148.0  \n",
       "2  Drejtoria Rajonale Tatimore Durres    2019-03-01  SALARY 03/19    114936.0  \n",
       "3                      Nettogehaelter    2019-03-01  SALARY 03/19    322773.0  \n",
       "4  Drejtoria Rajonale Tatimore Durres    2019-04-01  SALARY 04/19    132482.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################# building and executing the query ####################\n",
    "q = pql.PQL()\n",
    "# case key column names start with underscore\n",
    "for col in case_key:\n",
    "    q += pql.PQLColumn(col,'_' + col.replace('\"','').replace('.','_'))\n",
    "\n",
    "# add data columns to query\n",
    "for col in data_columns:\n",
    "    q += pql.PQLColumn(col,col.replace('\"','').replace('.','_'))\n",
    "    \n",
    "    \n",
    "# add filters to query \n",
    "for f in pql_filters:\n",
    "    q += pql.PQLFilter(f)\n",
    "\n",
    "# pull data\n",
    "df = dm._get_data_frame(q)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LFA1_NAME1_similar': {'LFA1_NAME1': \"String(column, column,label=column,threshold=0.85,method='jarowinkler')\",\n",
       "  'BKPF_TS_BLDAT': 'block',\n",
       "  'BKPF_XBLNR': 'block',\n",
       "  'BSEG_WRBTR': 'block'},\n",
       " 'BKPF_TS_BLDAT_similar': {'BKPF_TS_BLDAT': 'Date(column, column, label=column)',\n",
       "  'LFA1_NAME1': 'block',\n",
       "  'BKPF_XBLNR': 'block',\n",
       "  'BSEG_WRBTR': 'block'},\n",
       " 'BKPF_XBLNR_similar': {'BKPF_XBLNR': 'OneAddedCharacter(column, column)',\n",
       "  'LFA1_NAME1': 'block',\n",
       "  'BKPF_TS_BLDAT': 'block',\n",
       "  'BSEG_WRBTR': 'block'},\n",
       " 'BSEG_WRBTR_similar': {'BSEG_WRBTR': 'Numeric(column, column, method=\"linear\", offset=0.0, scale=10, label=column)',\n",
       "  'LFA1_NAME1': 'block',\n",
       "  'BKPF_TS_BLDAT': 'block',\n",
       "  'BKPF_XBLNR': 'block'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################# creating search patterns ####################################\n",
    "patterns = {}\n",
    "for col in df.columns:\n",
    "        if col == 'LFA1_NAME1':\n",
    "            patterns.update({col + \"_similar\" : {col: \"String(column, column,label=column,threshold=0.85,method='jarowinkler')\"}})\n",
    "        elif col == \"BKPF_XBLNR\":\n",
    "            patterns.update({col + \"_similar\" : {col: \"OneAddedCharacter(column, column)\"}})\n",
    "        elif col == \"BSEG_WRBTR\":\n",
    "            patterns.update({col + \"_similar\" : {col: \" \".join(\n",
    "                ('Numeric(column, column, method=\"linear\",',\n",
    "                 \"offset=0.0, scale=10, label=column)\")\n",
    "            )}})\n",
    "        elif col == \"BKPF_TS_BLDAT\":\n",
    "            patterns.update({col + \"_similar\" : {col: \"Date(column, column, label=column)\"}})\n",
    "        else:\n",
    "            continue\n",
    "        for c in df.columns:\n",
    "            if c != col and not c.startswith('_'):\n",
    "                patterns.get(col + \"_similar\").update({c: \"block\"})\n",
    "\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### importing the duplicate checker module ##############\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import recordlinkage\n",
    "from recordlinkage.compare import String, Date, Numeric\n",
    "from recordlinkage.preprocessing import clean, phonetic\n",
    "from recordlinkage.base import BaseCompareFeature\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "handler_normal = logging.StreamHandler(sys.stdout)\n",
    "handler_normal.addFilter(lambda log: 1 if log.levelno < 30 else 0)\n",
    "handler_normal.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s: %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"))\n",
    "handler_red = logging.StreamHandler(sys.stderr)\n",
    "handler_red.setLevel(logging.WARN)\n",
    "logging.basicConfig(level=logging.INFO, handlers=[handler_normal, handler_red])\n",
    "logger = logging.getLogger(\"Duplicate Checker\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DuplicateChecker:\n",
    "    df: pd.DataFrame\n",
    "    clean_strings: bool = True\n",
    "    get_exact: bool = False\n",
    "    get_similar: bool = True\n",
    "    get_common_errors: bool = True\n",
    "    matching_patterns: Dict = field(default_factory=dict)\n",
    "    methods: Dict = field(default_factory=dict)\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        \"\"\" Do everything that can be done at once for whole df \"\"\"\n",
    "        # empty reslts df\n",
    "        self.results = pd.DataFrame()\n",
    "        # drop duplicate entries and NAs\n",
    "        self._preprocess_df()\n",
    "        # remove timezone info\n",
    "        self._fix_dates()\n",
    "        # set matching patterns\n",
    "        if not self.methods:\n",
    "            self.methods = self._generate_methods()\n",
    "        if not self.matching_patterns:\n",
    "            self.matching_patterns = self._generate_patterns(self.methods)\n",
    "\n",
    "        # instanciate record linkage comparer objects base on patterns   \n",
    "        self.linkers = {}\n",
    "        for pattern_name, current_pattern in self.matching_patterns.items():\n",
    "            # needed inputs to build record linkage objects\n",
    "            blocked_columns = []\n",
    "            n_comparison_columns = 0\n",
    "            indexer = recordlinkage.Index()\n",
    "            comparer = recordlinkage.Compare()\n",
    "            \n",
    "            # is there a blocked column ? \n",
    "            blocks_exist = False\n",
    "            for column in current_pattern:\n",
    "                if current_pattern[column] == \"block\":\n",
    "                    blocks_exist = True\n",
    "                    blocked_columns.append(column)\n",
    "                else:\n",
    "                    n_comparison_columns += 1\n",
    "                    exec(\"comparer.add(\" + current_pattern[column] + \")\")\n",
    "\n",
    "            if blocks_exist:\n",
    "                indexer.block(blocked_columns)\n",
    "            else:\n",
    "                indexer.full()\n",
    "            # append comparison objects to linkers\n",
    "            self.linkers.update({\n",
    "                pattern_name: {\"indexer\":indexer,\n",
    "                               'blocked_columns': blocked_columns, \n",
    "                               'n_comparison_columns': n_comparison_columns,\n",
    "                               'comparer': comparer}\n",
    "                })\n",
    "\n",
    "    def run(self):\n",
    "        # 1) get exact matches\n",
    "        if self.get_exact:\n",
    "            df = self._get_exact_matches()\n",
    "            self.results = self.results.append(df)\n",
    "        # drop duplicates for further investigation\n",
    "        self._drop_duplicates()   \n",
    "         # 2) get common known errors like I instead of 1\n",
    "        if self.get_common_errors:\n",
    "            df = self._get_exact_matches(convert_common_errors=True)\n",
    "            if len(df) > 0:\n",
    "                df.Group_ID = df.Group_ID.str.replace(\n",
    "                    \"Exact\", \"Similar_symbol\")\n",
    "            self.results = self.results.append(df)\n",
    "        # remove all special characters\n",
    "        if self.clean_strings:\n",
    "            self._clean_strings()\n",
    "            \n",
    "        # TODO: Insert chunking here! \n",
    "        \n",
    "        # 2) get patterns from from matching patterns\n",
    "        if self.get_similar:\n",
    "            df = self._get_similar_matches(patterns=self.matching_patterns)\n",
    "            self.results = self.results.append(df)\n",
    "        \n",
    "        # FINAL: \n",
    "        # check if duplicates found\n",
    "        if self.results.empty:\n",
    "            self.group_counts = pd.Series({\"Total\": 0}, name=\"# Groups\")\n",
    "            print(self.group_counts)\n",
    "            return self.results\n",
    "        \n",
    "        # count groups and return results\n",
    "        self.group_counts = (\n",
    "            self.results.Group_ID.rename(\n",
    "                \"# Groups\").drop_duplicates().str.replace(pat=\"\"\"ID.*\"\"\", repl=\"\", regex=True).value_counts()\n",
    "        )\n",
    "        self.group_counts[\"Total\"] = sum(self.group_counts)\n",
    "        return self.results.merge(self.df, on=self.case_columns).sort_values(\"Group_ID\")\n",
    "\n",
    "    def _preprocess_df(self):\n",
    "        logger.info('Preprocessing DataFrame')\n",
    "        \"\"\"Drop duplicate and separate into case and data columns\"\"\"\n",
    "        # drop rows which contain any na value\n",
    "        self.df.dropna(inplace=True)\n",
    "        # drop rows which are 100 % same (key + data columns)\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # access case key\n",
    "        self.case_columns = list(\n",
    "            filter(lambda x: x[0] == \"_\", self.df.columns))\n",
    "        # drop rows with same key\n",
    "        self.df.drop_duplicates(self.case_columns, inplace=True)\n",
    "        \n",
    "        # data columns to be checked for duplciates\n",
    "        self.compare_columns = list(\n",
    "            filter(lambda x: x[0] != \"_\", self.df.columns))\n",
    "        self.cases = self.df[self.case_columns].copy()\n",
    "        self.data = self.df[self.compare_columns].copy()\n",
    "\n",
    "    def _get_exact_matches(self, keep_duplicates=False, convert_common_errors=False):\n",
    "        \"\"\" calculate exact matches or calculate matches after common error conversion\"\"\"\n",
    "        # replace common errors and check if duplicate\n",
    "        if convert_common_errors:\n",
    "            logger.info('Searching for common error ...')\n",
    "            dups = self._convert_common_errors()\n",
    "            word = \"Similar_symbol\"\n",
    "        # select indizes of duplicated data entries (all data columns are the same)\n",
    "        else:\n",
    "            logger.info('Searching for exact duplicates ...')\n",
    "            dups = self.df[self.df.duplicated(self.compare_columns, keep=False)]\n",
    "            word = 'Exact_match'\n",
    "        \n",
    "        # auxilliary function\n",
    "        def group_duplicate_index(df):\n",
    "            \"\"\" return indices of duplicates rows\"\"\"\n",
    "            a = df.values\n",
    "            sidx = np.lexsort(a.T)\n",
    "            b = a[sidx]\n",
    "\n",
    "            m = np.concatenate(([False], (b[1:] == b[:-1]).all(1), [False] ))\n",
    "            idx = np.flatnonzero(m[1:] != m[:-1])\n",
    "            I = df.index[sidx].tolist()       \n",
    "            return [I[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]\n",
    "        \n",
    "        # get list of list of matching indices\n",
    "        groups = group_duplicate_index(dups[self.compare_columns])\n",
    "        # help df to store matches in\n",
    "        temp_df = dups[self.case_columns].copy()\n",
    "        temp_df[\"Group_ID\"] = \"\"\n",
    "        \n",
    "        # match groups\n",
    "        def assign_group(g):\n",
    "            s = temp_df.loc[g,:].agg('-'.join, axis=1).tolist()\n",
    "            s = \",\".join(s)\n",
    "            group_name = word + \" IDs:\" + f\"({s})\"\n",
    "            return list(g), group_name\n",
    "        \n",
    "        # parallel group assigment\n",
    "        for r in Parallel(n_jobs=-1, verbose=10)(delayed(assign_group)(g) for g in groups):\n",
    "            temp_df.loc[r[0], \"Group_ID\"] = r[1]\n",
    "\n",
    "    \n",
    "        return temp_df[temp_df[\"Group_ID\"] != \"\"]\n",
    "\n",
    "    def _get_similar_matches(self, methods=None, patterns=None):\n",
    "        \"\"\"Finds groups of rows where comparison columns are similar.\n",
    "\n",
    "        :param methods:\n",
    "            Dictionary of recordlinkage compare classes matched to pandas dtypes.\n",
    "        :param patterns:\n",
    "            Dictionary of column combinations and methods for each column.\n",
    "\n",
    "        :return: Dataframe with ID columns of matches with match group ID.\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(f'Searching for fuzzy matches ...')\n",
    "        data_to_compare = self.df[self.compare_columns].drop_duplicates()\n",
    "        possible_duplicates = pd.DataFrame()\n",
    "\n",
    "        def run_comparer(df_chunk,pattern_name):\n",
    "            # Run record linkage comparer\n",
    "            logger.info(f'Searching {df_chunk.shape[0]} for Pattern: {pattern_name}')\n",
    "            link = self.linkers.get(pattern_name)\n",
    "            print(df_chunk.shape)\n",
    "            data_links = link.get('indexer').index(df_chunk)\n",
    "            results = link.get('comparer').compute(data_links, df_chunk)\n",
    "            matches = results[results.sum(axis=1) > link.get('n_comparison_columns') - 1]\n",
    "            # create group id for pairs found\n",
    "            group_ids = self._determine_groups(pattern_name, matches)\n",
    "            groups = self.cases.merge(\n",
    "                group_ids, how=\"inner\", left_index=True, right_index=True)\n",
    "            return groups\n",
    "        \n",
    "        \n",
    "        # create chunks of data first and then parallelize running through them\n",
    "        chunks = [] # list of tuples\n",
    "        \n",
    "        for pattern_name, link in self.linkers.items():\n",
    "            # keep only those who have duplicates on blocked columns\n",
    "            link_df = data_to_compare[data_to_compare.duplicated(subset=link.get('blocked_columns'),keep=False)]\n",
    "            # number of items to compare\n",
    "            n = link_df.shape[0]\n",
    "            logger.info(f'Number of cases searched for Pattern {pattern_name}: {n}')\n",
    "            # max_number items to compare at once\n",
    "            max_size = 20000\n",
    "            print(link_df.shape)\n",
    "            if n > max_size:\n",
    "                chunk_number = round(n / max_size) + 1\n",
    "                link_df = link_df.sort_values(link.get('blocked_columns'))\n",
    "                link_df['C'] = np.arange(len(link_df))\n",
    "                link_df[\"quant\"] = pd.cut(link_df['C'], chunk_number * 10, labels=False, duplicates=\"drop\")\n",
    "                \n",
    "                for i in range(0, chunk_number):\n",
    "                    chunks += [(link_df[link_df[\"quant\"].isin(list(range( (i*10) -1 , (i+1)* 10 + 1))) ].drop([\"quant\",'C'], axis=1),pattern_name)]\n",
    "                    \n",
    "            else:\n",
    "                chunks += [(link_df, pattern_name)]\n",
    "    \n",
    "        # parallel execution of matching\n",
    "        for r in Parallel(n_jobs=-1, verbose=10)(delayed(run_comparer)(chunk[0],chunk[1]) for chunk in chunks):\n",
    "            possible_duplicates = possible_duplicates.append(r)\n",
    "            logger.info(r.shape)\n",
    "        return possible_duplicates\n",
    "\n",
    "    def _convert_common_errors(self, columns=[\"all\"]):\n",
    "        \"\"\"String columns are converted to phonetic using recordlinkage.\"\"\"\n",
    "        df = self.df[~self.df.duplicated(self.compare_columns,keep=False)].copy()\n",
    "        common_errors = {\n",
    "            \"B\": \"!\",\n",
    "            \"8\": \"!\",\n",
    "            \"b\": \"!\",\n",
    "            \"G\": \"@\",\n",
    "            \"6\": \"@\",\n",
    "            \"g\": \"@\",\n",
    "            \"i\": \"$\",\n",
    "            \"I\": \"$\",\n",
    "            \"1\": \"$\",\n",
    "            \"l\": \"$\",\n",
    "            \"0\": \":\",\n",
    "            \"O\": \":\",\n",
    "            \"Q\": \":\",\n",
    "            \"o\": \":\",\n",
    "            \"q\": \":\",\n",
    "            \"D\": \":\",\n",
    "            \"d\": \":\",\n",
    "            \"S\": \">\",\n",
    "            \"s\": \">\",\n",
    "            \"5\": \">\",\n",
    "            \"z\": \"<\",\n",
    "            \"Z\": \"<\",\n",
    "            \"2\": \"<\",\n",
    "        }\n",
    "        # choose cols to perform replace on\n",
    "        cols_to_use = []\n",
    "        for column in self.df[self.compare_columns].select_dtypes(include=[\"object\"]):\n",
    "            if columns == [\"all\"] or column in columns:\n",
    "                cols_to_use +=[column]\n",
    "\n",
    "        # replacement of characters for each column\n",
    "        def replace_commons(column):\n",
    "            s = df[column].copy()\n",
    "            for key, value in common_errors.items():\n",
    "                s = s.str.replace(key, value)\n",
    "            #s[s.str.contains(r'[A-Z]{1,3}\\s?\\-?\\s?[0-9]{4,}$')] = s[s.str.contains(r'[A-Z]{1,3}\\s?\\-?\\s?[0-9]{4,}$')].str.replace(r'[^0-9]','')\n",
    "            return column, s\n",
    "        \n",
    "        # parallel execution\n",
    "        for r in Parallel(n_jobs=-1, verbose=10)(delayed(replace_commons)(chunk) for chunk in cols_to_use):\n",
    "            df[r[0]] = r[1]\n",
    "        \n",
    "        return df[df.duplicated(subset=self.compare_columns,keep=False)]\n",
    "\n",
    "    def _clean_strings(self):\n",
    "        \"\"\"String columns are cleaned using recordlinkage clean method.\"\"\"\n",
    "        for column in self.data.select_dtypes(include=[\"object\"]):\n",
    "            self.data[column] = clean(\n",
    "                self.data[column], strip_accents=\"unicode\", remove_brackets=False, replace_by_none=\"[^ A-Za-z0-9]+\"\n",
    "            )\n",
    "\n",
    "    def _drop_duplicates(self):\n",
    "        self.data = self.data.drop_duplicates()\n",
    "\n",
    "    def _fix_dates(self):\n",
    "        \"\"\"\n",
    "        Timezone information is removed from datetime columns, because\n",
    "        recordlinkage doesn't accept datetime columns with timezones.\n",
    "        \"\"\"\n",
    "        for column in self.data.select_dtypes(include=[\"datetimetz\"]):\n",
    "            self.data[column] = self.data[column].dt.tz_convert(None)\n",
    "\n",
    "    def _generate_methods(self):\n",
    "        \"\"\"Recordlinkage comparer objects are matched to pandas datatypes.\"\"\"\n",
    "        methods = {}\n",
    "        # TODO differentiate between string columns and mix columns \n",
    "        for dtype in [\"object\"]:\n",
    "            methods[dtype] = \"OneAddedCharacter(column, column)\"\n",
    "        for dtype in [\"int64\", \"float64\", \"int32\", \"float32\"]:\n",
    "            methods[dtype] = \" \".join(\n",
    "                ('Numeric(column, column, method=\"linear\",',\n",
    "                 \"offset=0.0, scale=10, label=column)\")\n",
    "            )\n",
    "        for dtype in [\"datetime\", \"datetime64\", \"datetime64[ns]\"]:\n",
    "            methods[dtype] = \"Date(column, column, label=column)\"\n",
    "        print(\"Data comparison method library used: \" + str(methods))\n",
    "        return methods\n",
    "\n",
    "    def _generate_patterns(self, methods):\n",
    "        \"\"\"\n",
    "        Generates search patterns for each column. For each column the other\n",
    "        columns are blocked, this means that we only search for similarities\n",
    "        in this column between rows where other columns match exactly.\n",
    "\n",
    "        :param methods (dict): a comparison method for each datatype in data\n",
    "        \"\"\"\n",
    "\n",
    "        patterns = {}\n",
    "        for column in self.data:\n",
    "            dtype = str(self.data[column].dtype)\n",
    "            pattern_name = column + \"_similar\"\n",
    "            patterns.update({pattern_name: {column: methods.get(dtype)}})\n",
    "            if not patterns[pattern_name][column]:\n",
    "                raise TypeError(\n",
    "                    \"Search pattern problem: no method found for type \" + dtype)\n",
    "            for other_column in self.data.drop(column, axis=1):\n",
    "                patterns[pattern_name].update({other_column: \"block\"})\n",
    "        print(\"Search patterns used: \" + str(patterns))\n",
    "        return patterns\n",
    "\n",
    "    def _determine_groups(self, pattern, matches):\n",
    "        \"\"\"Matches from recordlinkage are turned into groups with a unique ID.\"\"\"\n",
    "\n",
    "        group = 0\n",
    "\n",
    "        temp_df = pd.DataFrame(index=self.cases.index)\n",
    "        temp_df[\"Group_ID\"] = \"\"\n",
    "        for index, _ in matches.iterrows():\n",
    "            s = self.cases.loc[list(index), :].agg('-'.join, axis=1).tolist()\n",
    "            s = \",\".join(s)\n",
    "            group_name = pattern + \" IDs:\" + f\"({s})\"\n",
    "            temp_df.loc[list(index), \"Group_ID\"] = group_name\n",
    "        return temp_df[temp_df[\"Group_ID\"] != \"\"]\n",
    "\n",
    "\n",
    "class OneAddedCharacter(BaseCompareFeature):\n",
    "    \"\"\"Compare the record pairs by checking whether they are exactly equal\n",
    "    except for 1 extra character.\n",
    "\n",
    "    The similarity is 1 in case of agreement and 0 otherwise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    left_on : str or int\n",
    "        Field name to compare in left DataFrame.\n",
    "    right_on : str or int\n",
    "        Field name to compare in right DataFrame.\n",
    "    agree_value : float, str, numpy.dtype\n",
    "        The value when two records are identical. Default 1.\n",
    "    disagree_value : float, str, numpy.dtype\n",
    "        The value when two records are not identical.\n",
    "    missing_value : float, str, numpy.dtype\n",
    "        The value for a comparison with a missing value. Default 0.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"one-added-character\"\n",
    "    description = \"Compare attributes of record pairs.\"\n",
    "\n",
    "    def __init__(self, left_on, right_on, agree_value=1, disagree_value=0, missing_value=0, label=None):\n",
    "        super(OneAddedCharacter, self).__init__(left_on, right_on, label=label)\n",
    "\n",
    "        self.agree_value = agree_value\n",
    "        self.disagree_value = disagree_value\n",
    "        self.missing_value = missing_value\n",
    "\n",
    "    def _compute_vectorized(self, s_left, s_right):\n",
    "\n",
    "        compare = pd.DataFrame(data={\"s_left\": s_left, \"s_right\": s_right})\n",
    "        compare[\"result\"] = self.disagree_value\n",
    "        compare[\"len_left\"] = s_left.map(len)\n",
    "        compare[\"len_right\"] = s_right.map(len)\n",
    "        compare[\"diff\"] = compare[\"len_left\"] - compare[\"len_right\"]\n",
    "        compare[\"matches\"] = 0\n",
    "\n",
    "        def check_strings(s1, s2):\n",
    "            left = 0\n",
    "            right = 0\n",
    "            for f1, f2 in zip(s1[:-1], s2):\n",
    "                if f1 == f2:\n",
    "                    left += 1\n",
    "                else:\n",
    "                    for b1, b2 in zip(s1[:0:-1], s2[::-1]):\n",
    "                        if b1 == b2:\n",
    "                            right += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    break\n",
    "            if right == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return left + right\n",
    "\n",
    "        compare.loc[(compare[\"diff\"] == 1) & (compare[\"len_right\"] > 0), \"matches\"] = compare.loc[\n",
    "            (compare[\"diff\"] == 1) & (compare[\"len_right\"] > 0)\n",
    "        ].apply(lambda x: check_strings(x[\"s_left\"], x[\"s_right\"]), axis=1)\n",
    "        compare.loc[(compare[\"diff\"] == -1) & (compare[\"len_left\"] > 0), \"matches\"] = compare.loc[\n",
    "            (compare[\"diff\"] == -1) & (compare[\"len_left\"] > 0)\n",
    "        ].apply(lambda x: check_strings(x[\"s_right\"], x[\"s_left\"]), axis=1)\n",
    "\n",
    "        compare.loc[\n",
    "            (compare[\"len_right\"] > 0) & (\n",
    "                compare[\"len_right\"] <= compare[\"matches\"]), \"result\"\n",
    "        ] = self.agree_value\n",
    "        compare.loc[\n",
    "            (compare[\"len_left\"] > 0) & (\n",
    "                compare[\"len_left\"] <= compare[\"matches\"]), \"result\"\n",
    "        ] = self.agree_value\n",
    "\n",
    "        # Only when disagree value is not identical with the missing value\n",
    "        if self.disagree_value != self.missing_value:\n",
    "            compare[(s_left.isnull() | s_right.isnull())] = self.missing_value\n",
    "\n",
    "        return compare[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:15:09 - Duplicate Checker: Preprocessing DataFrame\n",
      "Data comparison method library used: {'object': 'OneAddedCharacter(column, column)', 'int64': 'Numeric(column, column, method=\"linear\", offset=0.0, scale=10, label=column)', 'float64': 'Numeric(column, column, method=\"linear\", offset=0.0, scale=10, label=column)', 'int32': 'Numeric(column, column, method=\"linear\", offset=0.0, scale=10, label=column)', 'float32': 'Numeric(column, column, method=\"linear\", offset=0.0, scale=10, label=column)', 'datetime': 'Date(column, column, label=column)', 'datetime64': 'Date(column, column, label=column)', 'datetime64[ns]': 'Date(column, column, label=column)'}\n"
     ]
    }
   ],
   "source": [
    "######################## instanciate duplicate checker ####################                \n",
    "dc = DuplicateChecker(\n",
    "            df,\n",
    "            clean_strings = True,\n",
    "            get_exact= True,\n",
    "            get_similar= True,\n",
    "            get_common_errors= True,\n",
    "            matching_patterns=patterns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:15:13 - Duplicate Checker: Searching for exact duplicates ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0183s.) Setting batch_size=20.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1222s.) Setting batch_size=64.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 747 out of 747 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:15:15 - Duplicate Checker: Searching for common error ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   0 out of   0 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:15:25 - Duplicate Checker: Searching for fuzzy matches ...\n",
      "2020-05-18 10:15:26 - Duplicate Checker: Number of cases searched for Pattern LFA1_NAME1_similar: 1257\n",
      "(1257, 4)\n",
      "2020-05-18 10:15:26 - Duplicate Checker: Number of cases searched for Pattern BKPF_TS_BLDAT_similar: 257\n",
      "(257, 4)\n",
      "2020-05-18 10:15:26 - Duplicate Checker: Number of cases searched for Pattern BKPF_XBLNR_similar: 3219\n",
      "(3219, 4)\n",
      "2020-05-18 10:15:26 - Duplicate Checker: Number of cases searched for Pattern BSEG_WRBTR_similar: 82664\n",
      "(82664, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=2)]: Done   6 out of   8 | elapsed:    9.1s remaining:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-18 10:15:37 - Duplicate Checker: (66, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (2, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (10, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (88, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (184, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (112, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (104, 6)\n",
      "2020-05-18 10:15:37 - Duplicate Checker: (117, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   8 out of   8 | elapsed:   10.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   8 out of   8 | elapsed:   10.4s finished\n"
     ]
    }
   ],
   "source": [
    "res = dc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_BSEG_MANDT</th>\n",
       "      <th>_BSEG_BUKRS</th>\n",
       "      <th>_BSEG_BELNR</th>\n",
       "      <th>_BSEG_GJAHR</th>\n",
       "      <th>_BSEG_BUZEI</th>\n",
       "      <th>Group_ID</th>\n",
       "      <th>LFA1_NAME1</th>\n",
       "      <th>BKPF_TS_BLDAT</th>\n",
       "      <th>BKPF_XBLNR</th>\n",
       "      <th>BSEG_WRBTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>010</td>\n",
       "      <td>1HKG</td>\n",
       "      <td>2000000892</td>\n",
       "      <td>2019</td>\n",
       "      <td>001</td>\n",
       "      <td>BKPF_TS_BLDAT_similar IDs:(010-1HKG-2000001057...</td>\n",
       "      <td>WONG KWOK LEARN BALDWIN</td>\n",
       "      <td>2019-06-20</td>\n",
       "      <td>ANDREAS LUETZ</td>\n",
       "      <td>98000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>010</td>\n",
       "      <td>1HKG</td>\n",
       "      <td>2000001057</td>\n",
       "      <td>2019</td>\n",
       "      <td>001</td>\n",
       "      <td>BKPF_TS_BLDAT_similar IDs:(010-1HKG-2000001057...</td>\n",
       "      <td>WONG KWOK LEARN BALDWIN</td>\n",
       "      <td>2019-07-20</td>\n",
       "      <td>ANDREAS LUETZ</td>\n",
       "      <td>98000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>010</td>\n",
       "      <td>1ASU</td>\n",
       "      <td>2500000032</td>\n",
       "      <td>2019</td>\n",
       "      <td>001</td>\n",
       "      <td>BKPF_XBLNR_similar IDs:(010-1ASU-2500000032-20...</td>\n",
       "      <td>DIRECCI�N NACIONAL DE AERONAUTICA C</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>001-025-0030267</td>\n",
       "      <td>794.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>010</td>\n",
       "      <td>1ASU</td>\n",
       "      <td>3500000070</td>\n",
       "      <td>2019</td>\n",
       "      <td>001</td>\n",
       "      <td>BKPF_XBLNR_similar IDs:(010-1ASU-2500000032-20...</td>\n",
       "      <td>DIRECCI�N NACIONAL DE AERONAUTICA C</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>001-025-00300267</td>\n",
       "      <td>794.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>010</td>\n",
       "      <td>1HV0</td>\n",
       "      <td>2020001380</td>\n",
       "      <td>2019</td>\n",
       "      <td>002</td>\n",
       "      <td>BKPF_XBLNR_similar IDs:(010-1HV0-2020000626-20...</td>\n",
       "      <td>CIVIL AVIATION ADMINISTRATION</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>0319299304</td>\n",
       "      <td>1503859.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _BSEG_MANDT _BSEG_BUKRS _BSEG_BELNR _BSEG_GJAHR _BSEG_BUZEI  \\\n",
       "1591         010        1HKG  2000000892        2019         001   \n",
       "1592         010        1HKG  2000001057        2019         001   \n",
       "1600         010        1ASU  2500000032        2019         001   \n",
       "1597         010        1ASU  3500000070        2019         001   \n",
       "1593         010        1HV0  2020001380        2019         002   \n",
       "\n",
       "                                               Group_ID  \\\n",
       "1591  BKPF_TS_BLDAT_similar IDs:(010-1HKG-2000001057...   \n",
       "1592  BKPF_TS_BLDAT_similar IDs:(010-1HKG-2000001057...   \n",
       "1600  BKPF_XBLNR_similar IDs:(010-1ASU-2500000032-20...   \n",
       "1597  BKPF_XBLNR_similar IDs:(010-1ASU-2500000032-20...   \n",
       "1593  BKPF_XBLNR_similar IDs:(010-1HV0-2020000626-20...   \n",
       "\n",
       "                               LFA1_NAME1 BKPF_TS_BLDAT        BKPF_XBLNR  \\\n",
       "1591              WONG KWOK LEARN BALDWIN    2019-06-20     ANDREAS LUETZ   \n",
       "1592              WONG KWOK LEARN BALDWIN    2019-07-20     ANDREAS LUETZ   \n",
       "1600  DIRECCI�N NACIONAL DE AERONAUTICA C    2019-08-13   001-025-0030267   \n",
       "1597  DIRECCI�N NACIONAL DE AERONAUTICA C    2019-08-13  001-025-00300267   \n",
       "1593        CIVIL AVIATION ADMINISTRATION    2019-01-18        0319299304   \n",
       "\n",
       "      BSEG_WRBTR  \n",
       "1591    98000.00  \n",
       "1592    98000.00  \n",
       "1600      794.32  \n",
       "1597      794.32  \n",
       "1593  1503859.11  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_BSEG_MANDT</th>\n",
       "      <th>_BSEG_BUKRS</th>\n",
       "      <th>_BSEG_BELNR</th>\n",
       "      <th>_BSEG_GJAHR</th>\n",
       "      <th>_BSEG_BUZEI</th>\n",
       "      <th>Group_ID</th>\n",
       "      <th>LFA1_NAME1</th>\n",
       "      <th>BKPF_TS_BLDAT</th>\n",
       "      <th>BKPF_XBLNR</th>\n",
       "      <th>BSEG_WRBTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BKPF_TS_BLDAT_similar</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BKPF_XBLNR_similar</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSEG_WRBTR_similar</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact_match</th>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFA1_NAME1_similar</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _BSEG_MANDT  _BSEG_BUKRS  _BSEG_BELNR  _BSEG_GJAHR  \\\n",
       "Group_ID                                                                     \n",
       "BKPF_TS_BLDAT_similar             2            2            2            2   \n",
       "BKPF_XBLNR_similar               10           10           10           10   \n",
       "BSEG_WRBTR_similar              605          605          605          605   \n",
       "Exact_match                    1522         1522         1522         1522   \n",
       "LFA1_NAME1_similar               66           66           66           66   \n",
       "\n",
       "                        _BSEG_BUZEI  Group_ID  LFA1_NAME1  BKPF_TS_BLDAT  \\\n",
       "Group_ID                                                                   \n",
       "BKPF_TS_BLDAT_similar             2         2           2              2   \n",
       "BKPF_XBLNR_similar               10        10          10             10   \n",
       "BSEG_WRBTR_similar              605       605         605            605   \n",
       "Exact_match                    1522      1522        1522           1522   \n",
       "LFA1_NAME1_similar               66        66          66             66   \n",
       "\n",
       "                        BKPF_XBLNR  BSEG_WRBTR  \n",
       "Group_ID                                        \n",
       "BKPF_TS_BLDAT_similar            2           2  \n",
       "BKPF_XBLNR_similar              10          10  \n",
       "BSEG_WRBTR_similar             605         605  \n",
       "Exact_match                   1522        1522  \n",
       "LFA1_NAME1_similar              66          66  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicate groups\n",
    "res.groupby(res['Group_ID'].str.replace(r'IDs.*','')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1) PUSH TABLE\n",
    "dm.push_table(res, 'DUPLICATE_INVOICES', if_exists='replace', reload_datamodel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2) create case_id query with REMAP VALUES to be used directly in analysis\n",
    "cols = [ '_'+ c.replace('\"', \"\").replace(\".\", \"_\") for c in sorted(['BSEG.MANDT','BSEG.BUKRS','BSEG.BELNR','BSEG.GJAHR','BSEG.BUZEI'])]\n",
    "key = \"\"\" ||':'|| \"\"\".join(sorted(['BSEG.MANDT','BSEG.BUKRS','BSEG.BELNR','BSEG.GJAHR','BSEG.BUZEI']))\n",
    "\n",
    "# create unique id column\n",
    "for col in cols:\n",
    "    res[col] = res[col].fillna(\"\")\n",
    "res[\"case_key\"] = (res[cols]).agg(\":\".join, axis=1)\n",
    "s = f\"REMAP_VALUES( {key}, \"\n",
    "for g in res[\"Group_ID\"].unique():\n",
    "    k = res[res[\"Group_ID\"] == g][\"case_key\"].tolist()\n",
    "    for c in k:\n",
    "        s += f\"\"\" [ '{c}', '{g}' ], \"\"\"\n",
    "s += \" NULL)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMAP_VALUES( BSEG.BELNR ||':'|| BSEG.BUKRS ||':'|| BSEG.BUZEI ||':'|| BSEG.GJAHR ||':'|| BSEG.MANDT,  [ '0000075608:1000:002:2009:800', 'BKPF_TS_BLDAT_similar IDs:(800-1000-0000302108-2010-002,800-1000-0000075608-2009-002)' ],  [ '0000302108:1000:002:2010:800', 'BKPF_TS_BLDAT_similar IDs:(800-1000-0000302108-2010-002,800-1000-0000075608-2009-002)' ],  [ '0000030189:1000:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000030189-2009-001,800-1000-0000256689-2010-001)' ],  [ '0000256689:1000:001:2010:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000030189-2009-001,800-1000-0000256689-2010-001)' ],  [ '0000075753:1000:004:2009:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000075753-2009-004,800-1000-0000302253-2010-004)' ],  [ '0000302253:1000:004:2010:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000075753-2009-004,800-1000-0000302253-2010-004)' ],  [ '0000075859:1000:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000075859-2009-001,800-R100-0000226859-2009-001)' ],  [ '0000226859:R100:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000075859-2009-001,800-R100-0000226859-2009-001)' ],  [ '0000151121:1000:004:2003:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000151121-2003-004,800-1000-0000000121-2009-004)' ],  [ '0000000121:1000:004:2009:800', 'BSEG_WRBTR_similar IDs:(800-1000-0000151121-2003-004,800-1000-0000000121-2009-004)' ],  [ '0000075888:2000:004:2009:800', 'BSEG_WRBTR_similar IDs:(800-2000-0000075888-2009-004,800-1000-0000000388-2009-004)' ],  [ '0000000388:1000:004:2009:800', 'BSEG_WRBTR_similar IDs:(800-2000-0000075888-2009-004,800-1000-0000000388-2009-004)' ],  [ '0000338043:R100:001:2001:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000111543-2009-001,800-R100-0000338043-2001-001)' ],  [ '0000111543:3000:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000111543-2009-001,800-R100-0000338043-2001-001)' ],  [ '0000111634:3000:001:2010:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000111634-2010-001,800-3000-0000262634-2010-001)' ],  [ '0000262634:3000:001:2010:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000111634-2010-001,800-3000-0000262634-2010-001)' ],  [ '0000298866:3000:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000223366-2009-001,800-3000-0000298866-2009-001)' ],  [ '0000223366:3000:001:2009:800', 'BSEG_WRBTR_similar IDs:(800-3000-0000223366-2009-001,800-3000-0000298866-2009-001)' ],  [ '0000226893:R100:002:2009:800', 'BSEG_WRBTR_similar IDs:(800-R100-0000075893-2009-002,800-R100-0000226893-2009-002)' ],  [ '0000075893:R100:002:2009:800', 'BSEG_WRBTR_similar IDs:(800-R100-0000075893-2009-002,800-R100-0000226893-2009-002)' ],  [ '0000111631:3000:001:2010:800', 'BSEG_WRBTR_similar IDs:(800-R100-0000187131-2010-001,800-3000-0000111631-2010-001)' ],  [ '0000187131:R100:001:2010:800', 'BSEG_WRBTR_similar IDs:(800-R100-0000187131-2010-001,800-3000-0000111631-2010-001)' ],  [ '0000226492:R100:001:2009:800', 'Similar_symbol IDs:(800-R100-0000226492-2009-001,800-R100-0000150992-2010-001)' ],  [ '0000150992:R100:001:2010:800', 'Similar_symbol IDs:(800-R100-0000226492-2009-001,800-R100-0000150992-2010-001)' ],  NULL)\n"
     ]
    }
   ],
   "source": [
    "#copy this and put it into a variable in an Analysis\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
