{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "#_PARENT_DIR = dirname(dirname(dirname(abspath(__file__))))\n",
    "#sys.path.append(_PARENT_DIR)\n",
    "#from utils._utils import parse_celonis_url, get_user_data, get_logger              ### <--\n",
    "from pycelonis import get_celonis\n",
    "import argparse\n",
    "#logger = get_logger()\n",
    "def unify_tables(url, api_token, datapool, tables, tableSchema, function = 'null'):\n",
    "    \n",
    "    #loop through defined tables\n",
    "    #for table_name in tables:\n",
    "       \n",
    "    c = get_celonis(url, api_token)\n",
    "    #get_user_data(c, 'Table Unifier')\n",
    "    #url_options= parse_celonis_url(url)                                            ### <--\n",
    "    #pool = c.pools.find(url_options['id'])                                         ### <--\n",
    "    \n",
    "    pool = c.pools.find(datapool)                                                   ### <--\n",
    "\n",
    "    minReq = []\n",
    "    maxReq = []\n",
    "    sameTables = []\n",
    "    schemalist = tableSchema                                                        # added to extract required table schemas (in python list format)\n",
    "    columns = ''\n",
    "    select = ''\n",
    "    \n",
    "    # create or find global data job \n",
    "    try:\n",
    "        global_dj = pool.data_jobs.find('Unify Tables')\n",
    "    except:\n",
    "        global_dj = pool.create_data_job(name = 'Unify Tables')\n",
    "    \n",
    "    #loop through defined tables\n",
    "    for table_name in tables:\n",
    "    \n",
    "        # find tables to unify\n",
    "        x = 0\n",
    "        for schema in global_dj.tables:\n",
    "            for table in schema:\n",
    "                x = x + 1\n",
    "                if table['schemaName'] not in schemalist:\n",
    "                    continue\n",
    "                if table['name'] == table_name:\n",
    "                    sameTables.append(table)\n",
    "        \n",
    "        if len(sameTables) == 0:\n",
    "            print('TABLE', table_name, 'CAN NOT BE UNIFIED! Not existing in source systems!')\n",
    "            global_dj.create_transformation(name = \"Unify \" + table_name, statement = '-- no existing tables to unify in defined source systems')\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # create minimal column requirement\n",
    "            for col in sameTables[0]['columns']:\n",
    "                try:\n",
    "                    if col in sameTables[1]['columns']:                                              # ERROR CHECKING !\n",
    "                        minReq.append(col)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            \"\"\"\n",
    "            #only required if function = 'min'\n",
    "\n",
    "            if len(sameTables) > 2:\n",
    "                for table in sameTables[2:]:\n",
    "                    for col in minReq:\n",
    "                        if col not in table['columns']:\n",
    "                            minReq.pop(col)\n",
    "                # create transformation statement for minimal table requirement\n",
    "\n",
    "            if function == 'min':\n",
    "                for col in minReq:\n",
    "                    if(minReq.index(col) == 0):\n",
    "                        columns += '{}\\n'.format(col['name'])\n",
    "                    else:\n",
    "                        columns += ',{}\\n'.format(col['name'])\n",
    "                for table in sameTables:\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            \"\"\"            \n",
    "\n",
    "            # create transformation for statement for extension with null\n",
    "            if function == 'null':\n",
    "                #columns = ''\n",
    "                #select = ''\n",
    "                \n",
    "                for table in sameTables:\n",
    "                    for col in table['columns']:\n",
    "                        if col not in maxReq:\n",
    "                            maxReq.append(col)\n",
    "                for table in sameTables:\n",
    "                    for col in maxReq:\n",
    "                        if(maxReq.index(col) == 0):\n",
    "                            columns += \"'\" + table['schemaName'] + \"' \" + 'AS \"SCHEMA\"\\n' + ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                        else:\n",
    "                            columns += ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    columns = ''\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            # create transformation\n",
    "            #statement = 'DROP VIEW IF EXISTS \"{}_UNIFIED\";\\n\\nCreate VIEW \"{}_UNIFIED\" AS(\\n{}\\n);'.format(table_name, table_name, select)\n",
    "            statement = 'DROP VIEW IF EXISTS \"{}\";\\n\\nCreate VIEW \"{}\" AS(\\n{}\\n);'.format(table_name, table_name, select)                      ### <-- Unified\n",
    "            global_dj.create_transformation(name = \"Unify \" + table_name, statement = statement)\n",
    "            print('The union statement has been saved in the global data job \"Unify Tables\" for Table', table_name)\n",
    "            select = ''\n",
    "            sameTables = []\n",
    "            minReq = []\n",
    "            maxReq = []\n",
    "            \n",
    "            # reminder: datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-20 10:26:38 - pycelonis: Login successful! Hello josef.rieger@cbs-consulting.de\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table T052U\n"
     ]
    }
   ],
   "source": [
    "# URL to the Cloud Data Pool where the tables to unify are hosted.\n",
    "url = 'https://endresshauser.eu-3.celonis.cloud/'\n",
    "\n",
    "# Specify a valid API token for your Cloud Team.\n",
    "#Click on Team Settings -> Edit -> create or copy already inserted API key\n",
    "\n",
    "api_token = 'NDEzZDFkN2YtZjVmZC00MTE0LTg2ODAtNmZlZDM5YzU5YTEwOjBxR0xOSFhjdDQ4MFNDc0V1UDIwbldzNWd2VXFVeUZieCt5MjdzbUhySkRP'\n",
    "\n",
    "\n",
    "#To get the data pool id, you have to run the following code. To get the data\n",
    "#the ML User here called Global needs permission to request these informations\n",
    "#celonis = get_celonis()\n",
    "#celonis.pools by that you get the ID for the data pool \n",
    "\n",
    "# Specify a datapool -- O2C Data Pool ID\n",
    "datapool = '60442433-3dbb-4ce6-8ed3-e9bb7238c614'\n",
    "\n",
    "# Specify the table names for which the tables should be merged. \n",
    "#KONV is not Celonis standard\n",
    "\n",
    "tables =['T052U']\n",
    "\n",
    "# Specify 'min' if the unified table should be the minimal column requirement and 'null' if the not existing columns should be filled with null values.\n",
    "function = 'null'\n",
    "\n",
    "\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_575c3915-e50d-48c0-a472-da19db3eee29 = PCA042\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_8ae9a322-9256-4298-bc90-2a6af6e6b36f = PIN020\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_195c34a6-cf16-431b-88f5-a608f44a7c14 = PIN024\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_3bbd20b8-81ce-4068-bc77-212ddfb300c5 = PP1011\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_e67ac4d6-dd7d-4653-a043-cdd83c9728b7 = PP2004\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_5bf5be9d-f13f-48fc-a5c2-0741f85727ae = PP2005\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_ff5eeb0b-b3ab-401f-8dc0-e701c4fdbd2a = PS1007\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_62c67a31-0987-4424-b967-64680da71015 = PS1030\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_d897dfda-15fb-4fa9-968f-90db8694e7cd = PS1032\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_acc25658-cfd5-4696-b4af-aa112af23fba = PS1033\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_9d804771-eac4-4fde-b23e-39710d5a8b58 = PS1035\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_80493315-33aa-43ce-a702-f970bab30f6f = PS1036\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_6567a63c-c54f-4edf-a594-a6100441211e  = PS1038\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_1cadd211-41b9-4632-a006-ffe0b103f034  = PS1039\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_6caea70a-87a4-41cc-bd4c-61c4829a6860 = PS1040\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_926ef30e-b9b4-4931-887b-a42794341bbf = PS1044\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_4d74b0c1-007e-4650-a312-46237a1d746c = PSG045\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_9b65d55f-cd2e-4aa7-89ad-af9fac513a95 = PSG048\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_d0b7b155-1fe2-4d29-89f2-5cfe11acdd05 = PSG049\n",
    "#60442433-3dbb-4ce6-8ed3-e9bb7238c614_886a0dcb-393a-4184-abf7-fa6e53b6dc10 = PS1090\n",
    "\n",
    "# Specify the required table schemas in a python list\n",
    "#To acess the Schema Name you have to get the connection informations\n",
    "#pool.data_connections\n",
    "tableSchema = [\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_3bbd20b8-81ce-4068-bc77-212ddfb300c5',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_9d804771-eac4-4fde-b23e-39710d5a8b58',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_195c34a6-cf16-431b-88f5-a608f44a7c14',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_acc25658-cfd5-4696-b4af-aa112af23fba',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_ff5eeb0b-b3ab-401f-8dc0-e701c4fdbd2a',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_8ae9a322-9256-4298-bc90-2a6af6e6b36f',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_6567a63c-c54f-4edf-a594-a6100441211e',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_9b65d55f-cd2e-4aa7-89ad-af9fac513a95',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_5bf5be9d-f13f-48fc-a5c2-0741f85727ae',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_d0b7b155-1fe2-4d29-89f2-5cfe11acdd05',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_e67ac4d6-dd7d-4653-a043-cdd83c9728b7',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_6caea70a-87a4-41cc-bd4c-61c4829a6860',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_d897dfda-15fb-4fa9-968f-90db8694e7cd',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_926ef30e-b9b4-4931-887b-a42794341bbf',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_62c67a31-0987-4424-b967-64680da71015',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_4d74b0c1-007e-4650-a312-46237a1d746c',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_575c3915-e50d-48c0-a472-da19db3eee29',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_1cadd211-41b9-4632-a006-ffe0b103f034',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_80493315-33aa-43ce-a702-f970bab30f6f',\n",
    "'60442433-3dbb-4ce6-8ed3-e9bb7238c614_886a0dcb-393a-4184-abf7-fa6e53b6dc10'    \n",
    "]\n",
    "\n",
    "unify_tables(url = url, api_token = api_token, datapool = datapool, tables = tables, tableSchema = tableSchema, function = function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-20 10:26:50 - pycelonis: Login successful! The Application Key currently has access to 25 Analyses and to 4 Data Pools.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://endresshauser.eu-3.celonis.cloud/'\n",
    "\n",
    "# Specify a valid API token for your Cloud Team.\n",
    "#Click on Team Settings -> Edit -> create or copy already inserted API key\n",
    "\n",
    "api_token = 'NDEzZDFkN2YtZjVmZC00MTE0LTg2ODAtNmZlZDM5YzU5YTEwOjBxR0xOSFhjdDQ4MFNDc0V1UDIwbldzNWd2VXFVeUZieCt5MjdzbUhySkRP'\n",
    "\n",
    "\n",
    "#To get the data pool id, you have to run the following code. To get the data\n",
    "#the ML User here called Global needs permission to request these informations\n",
    "#celonis = get_celonis()\n",
    "#celonis.pools by that you get the ID for the data pool \n",
    "\n",
    "# Specify a datapool -- O2C Data Pool ID\n",
    "datapool = '60442433-3dbb-4ce6-8ed3-e9bb7238c614'\n",
    "\n",
    "c = get_celonis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-20 10:26:50 - pycelonis: Login successful! The Application Key currently has access to 25 Analyses and to 4 Data Pools.\n"
     ]
    }
   ],
   "source": [
    "  celonis = get_celonis(url, api_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = celonis.pools.find(datapool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DataConnection, id 575c3915-e50d-48c0-a472-da19db3eee29, name SAP ECC - PCA-042>,\n",
       "<DataConnection, id 3bbd20b8-81ce-4068-bc77-212ddfb300c5, name SAP ECC - PP1-011>,\n",
       "<DataConnection, id 5bf5be9d-f13f-48fc-a5c2-0741f85727ae, name SAP ECC - PP2-005>,\n",
       "<DataConnection, id 8ae9a322-9256-4298-bc90-2a6af6e6b36f, name SAP ECC - PIN-020>,\n",
       "<DataConnection, id e67ac4d6-dd7d-4653-a043-cdd83c9728b7, name SAP ECC - PP2-004>,\n",
       "<DataConnection, id 9b65d55f-cd2e-4aa7-89ad-af9fac513a95, name SAP ECC - PSG-048>,\n",
       "<DataConnection, id 4d74b0c1-007e-4650-a312-46237a1d746c, name SAP ECC - PSG-045>,\n",
       "<DataConnection, id d0b7b155-1fe2-4d29-89f2-5cfe11acdd05, name SAP ECC - PSG-049>,\n",
       "<DataConnection, id ff5eeb0b-b3ab-401f-8dc0-e701c4fdbd2a, name SAP ECC - PS1-007>,\n",
       "<DataConnection, id acc25658-cfd5-4696-b4af-aa112af23fba, name SAP ECC - PS1-033>,\n",
       "<DataConnection, id 195c34a6-cf16-431b-88f5-a608f44a7c14, name SAP ECC - PIN-024>,\n",
       "<DataConnection, id 62c67a31-0987-4424-b967-64680da71015, name SAP ECC - PS1-030>,\n",
       "<DataConnection, id d897dfda-15fb-4fa9-968f-90db8694e7cd, name SAP ECC - PS1-032>,\n",
       "<DataConnection, id 9d804771-eac4-4fde-b23e-39710d5a8b58, name SAP ECC - PS1-035>,\n",
       "<DataConnection, id 80493315-33aa-43ce-a702-f970bab30f6f, name SAP ECC - PS1-036>,\n",
       "<DataConnection, id 6567a63c-c54f-4edf-a594-a6100441211e, name SAP ECC - PS1-038>,\n",
       "<DataConnection, id 1cadd211-41b9-4632-a006-ffe0b103f034, name SAP ECC - PS1-039>,\n",
       "<DataConnection, id 6caea70a-87a4-41cc-bd4c-61c4829a6860, name SAP ECC - PS1-040>,\n",
       "<DataConnection, id 926ef30e-b9b4-4931-887b-a42794341bbf, name SAP ECC - PS1-044>,\n",
       "<DataConnection, id 886a0dcb-393a-4184-abf7-fa6e53b6dc10, name SAP ECC - PS1-090>,\n",
       "<DataConnection, id ebbba564-eaaf-47bb-bc1f-6aa74159dc06, name SAP ECC - PS1-010>,\n",
       "<DataConnection, id 42c107cd-a24b-45d5-88f7-e0488ead4e06, name SAP ECC - PS1-900>,]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.data_connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
